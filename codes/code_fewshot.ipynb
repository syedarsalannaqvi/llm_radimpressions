{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd315d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# # The raw URL of the .ipynb file on GitHub\n",
    "# file_url = 'https://raw.githubusercontent.com/facebookresearch/llama-recipes/main/examples/Prompt_Engineering_with_Llama_2.ipynb'\n",
    "\n",
    "# # The name of the file to save locally\n",
    "# local_filename = 'Prompt_Engineering_with_Llama_2.ipynb'\n",
    "\n",
    "# # Make a GET request to fetch the raw content of the notebook\n",
    "# response = requests.get(file_url)\n",
    "# response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "# # Open the local file in write-binary mode and write the contents\n",
    "# with open(local_filename, 'wb') as f:\n",
    "#     f.write(response.content)\n",
    "\n",
    "# print(f'Downloaded file saved as: {local_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9221915c",
   "metadata": {
    "id": "9221915c"
   },
   "outputs": [],
   "source": [
    "# Import all the required packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import sidetable\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6be15a5",
   "metadata": {
    "id": "a6be15a5"
   },
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with your API key\n",
    "api_key = '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'\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a271d8e",
   "metadata": {
    "id": "3a271d8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\m254356\\\\Dropbox\\\\Github\\\\llm_radimpressions\\\\data\\\\data_fewshot',\n",
       " 'C:\\\\Users\\\\m254356\\\\Dropbox\\\\Github\\\\llm_radimpressions\\\\data\\\\data_finetuning',\n",
       " 'C:\\\\Users\\\\m254356\\\\Dropbox\\\\Github\\\\llm_radimpressions\\\\data\\\\data_zeroshot',\n",
       " 'C:\\\\Users\\\\m254356\\\\Dropbox\\\\Github\\\\llm_radimpressions\\\\data\\\\main_files']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the data files\n",
    "path = r'C:\\Users\\m254356\\Dropbox\\Github\\llm_radimpressions\\data'\n",
    "\n",
    "# Creating a list of all the paths for files in different experiments\n",
    "paths = []\n",
    "for subfolders in os.listdir(path):\n",
    "    x = os.path.join(path, subfolders)\n",
    "    paths.append(x)\n",
    "    \n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e9f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the sampled datafile for 100 patients\n",
    "df_test = pd.read_csv(paths[0] + r'\\df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f5ae0f9",
   "metadata": {
    "id": "2f5ae0f9"
   },
   "outputs": [],
   "source": [
    "df_test['final_deid'] = df_test['final_deid'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3fa528",
   "metadata": {
    "id": "4b3fa528"
   },
   "outputs": [],
   "source": [
    "def remove_substring(text, substring):\n",
    "    index = text.find(substring)\n",
    "    if index != -1:\n",
    "        return text[:index]  # Return the text up to the found index\n",
    "    else:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe23389",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = \"I, the teaching physician\"\n",
    "df_test['final_deid'] = df_test['final_deid'].apply(lambda x: remove_substring(x, remove))\n",
    "\n",
    "remove = \"ATTESTATION\"\n",
    "df_test['final_deid'] = df_test['final_deid'].apply(lambda x: remove_substring(x, remove))\n",
    "\n",
    "remove = \"Critical results were communicated\"\n",
    "df_test['final_deid'] = df_test['final_deid'].apply(lambda x: remove_substring(x, remove))\n",
    "\n",
    "remove = \"Electronically Signed by \"\n",
    "df_test['final_deid'] = df_test['final_deid'].apply(lambda x: remove_substring(x, remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ac28128",
   "metadata": {
    "id": "7ac28128"
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "#     encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "924cbb71",
   "metadata": {
    "id": "924cbb71"
   },
   "outputs": [],
   "source": [
    "# Encoding name for the  model\n",
    "# encoding_name = \"gpt-4\" ## if encoding for a specific model is used\n",
    "encoding_name = 'cl100k_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fce1d5ee",
   "metadata": {
    "id": "fce1d5ee",
    "outputId": "7b9d702b-be49-47d3-dd70-1c9f39ee7041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the dataset: 187698\n"
     ]
    }
   ],
   "source": [
    "# Now calculate the number of tokens for each truncated string\n",
    "df_test['num_tokens'] = df_test['final_deid'].apply(lambda x: num_tokens_from_string(x, encoding_name))\n",
    "\n",
    "# Calculate the total number of tokens\n",
    "total_tokens = df_test['num_tokens'].sum()\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f05faf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"\"\" Identify if the following radiology impression text indicates outcomes: (1) any cancer, (2) progression/worsening, (3) response/improvement, (4) brain metastases, (5) bone/osseous metastases, (6) adrenal metastases, (7) liver/hepatic metastases, (8) lung/pulmonary metastases, (9) lymph node/nodal metastases, (10) peritoneal metastases. Answer in Yes or No. Do not give an explanation.\n",
    "\n",
    "EXAMPLE: \n",
    "An example of impression and output is given below:\n",
    "\n",
    "IMPRESSION: \n",
    "FDG-avid right upper lobe pleural based mass is compatible with lung malignancy with an extension into the ipsilateral hilar, mediastinal, and subcarinal lymph nodes.  2.   2.1 x 1.6 cm ground-glass opacity in the left upper lobe without significant FDG uptake, however, has increased in size since, and could represent low grade lung malignancy.  3.    Unchanged size of a left sided 1.8 x 1.6 cm breast nodule, though which is mildly FDG avid. In addition, mildly FDG-avid right axillary and subpectoral lymph nodes; given the history of bilateral breast cancer, these findings should be further evaluated.\n",
    "\n",
    "1.\tany cancer: Yes \n",
    "2.\tprogression: Yes\n",
    "3.\tresponse/improvement: No\n",
    "4.\tbrain metastases: No \n",
    "5.\tbone/osseous metastases: No \n",
    "6.\tadrenal metastases: No \n",
    "7.\tliver/hepatic metastases: No \n",
    "8.\tFor lung/pulmonary metastases: Yes\n",
    "9.\tFor lymph node/nodal metastases: Yes\n",
    "10.\tFor peritoneal metastases: No \"\"\"\n",
    "\n",
    "num_tokens_from_string(prompt_text, encoding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2cc9124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102860"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['total_num_tokens'] = df_test['num_tokens'] + 381\n",
    "df_test['total_num_tokens'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8554af5",
   "metadata": {
    "id": "d8554af5"
   },
   "outputs": [],
   "source": [
    "# def truncate_to_token_limit(string: str, encoding_name: str, max_tokens: int) -> str:\n",
    "#     \"\"\"Truncates a text string to a specified token limit.\"\"\"\n",
    "#     encoding = tiktoken.get_encoding(encoding_name)\n",
    "#     encoded_string = encoding.encode(string)\n",
    "\n",
    "#     # Truncate the encoded string to the max_tokens limit\n",
    "#     if len(encoded_string) > max_tokens:\n",
    "#         encoded_string = encoded_string[:max_tokens]\n",
    "\n",
    "#     # Decode back to string (if necessary, depending on how your encoding works)\n",
    "#     truncated_string = encoding.decode(encoded_string)\n",
    "#     return truncated_string\n",
    "\n",
    "\n",
    "# # Set your token limit\n",
    "# token_limit = 100\n",
    "# # Truncate each string in the DataFrame to the token limit\n",
    "# df_test['truncated_text'] = df_test['final_deid'].apply(lambda x: truncate_to_token_limit(x, encoding_name, token_limit))\n",
    "\n",
    "# # Now calculate the number of tokens for each truncated string\n",
    "# df_test['num_tokens'] = df_test['truncated_text'].apply(lambda x: num_tokens_from_string(x, encoding_name))\n",
    "\n",
    "# # Calculate the total number of tokens\n",
    "# total_tokens = df_test['num_tokens'].sum()\n",
    "\n",
    "# # Display results\n",
    "# print(df_test[['truncated_text', 'num_tokens']])\n",
    "\n",
    "# df_test = df_test.sample(n=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0268f7c5",
   "metadata": {
    "id": "0268f7c5"
   },
   "outputs": [],
   "source": [
    "# Function to create a single GPT-4 API call\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "def create_gpt4_call(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to analyze radiology reports.\"},\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        temperature=1e-12,\n",
    "        max_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6889f189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Identify if the following radiology impression text indicates outcomes: (1) any cancer, (2) progression/worsening, (3) response/improvement, (4) brain metastases, (5) bone/osseous metastases, (6) adrenal metastases, (7) liver/hepatic metastases, (8) lung/pulmonary metastases, (9) lymph node/nodal metastases, (10) peritoneal metastases. Answer in Yes or No. Do not give an explanation.\n",
      "\n",
      "EXAMPLE: \n",
      "An example of impression and output is given below:\n",
      "\n",
      "IMPRESSION: \n",
      "FDG-avid right upper lobe pleural based mass is compatible with lung malignancy with an extension into the ipsilateral hilar, mediastinal, and subcarinal lymph nodes.  2.   2.1 x 1.6 cm ground-glass opacity in the left upper lobe without significant FDG uptake, however, has increased in size since, and could represent low grade lung malignancy.  3.    Unchanged size of a left sided 1.8 x 1.6 cm breast nodule, though which is mildly FDG avid. In addition, mildly FDG-avid right axillary and subpectoral lymph nodes; given the history of bilateral breast cancer, these findings should be further evaluated.\n",
      "\n",
      "1.\tany cancer: Yes \n",
      "2.\tprogression: Yes\n",
      "3.\tresponse/improvement: No\n",
      "4.\tbrain metastases: No \n",
      "5.\tbone/osseous metastases: No \n",
      "6.\tadrenal metastases: No \n",
      "7.\tliver/hepatic metastases: No \n",
      "8.\tFor lung/pulmonary metastases: Yes\n",
      "9.\tFor lymph node/nodal metastases: Yes\n",
      "10.\tFor peritoneal metastases: No \n"
     ]
    }
   ],
   "source": [
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc20c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of conditions to be checked\n",
    "conditions = [\"any cancer\", \"progression/worsening\", \"response/improvement\", \n",
    "                  \"brain metastases\", \"bone/osseous metastases\", \"adrenal metastases\", \n",
    "                  \"liver/hepatic metastases\", \"lung/pulmonary metastases\", \n",
    "                  \"lymph node/nodal metastases\", \"peritoneal metastases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fca447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results = r'C:\\Users\\m254356\\Dropbox\\Github\\llm_radimpressions\\results\\gpt-4-0125-preview\\fewshot_baseline\\\\'\n",
    "\n",
    "# Function to classify impressions for the fixed set of conditions\n",
    "def classify_impressions(df, column_name, conditions, prompt_text, num):\n",
    "    \n",
    "    # Reset the index of the DataFrame before running the loop. \n",
    "    # This will ensure that the DataFrame has a simple integer-based index which should align with loop's index variable.\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Initialize a dictionary to store predictions\n",
    "    predictions = {condition: [] for condition in conditions}\n",
    "    \n",
    "    # Calculate token count for the static part of the prompt\n",
    "    static_prompt_tokens = num_tokens_from_string(prompt_text, \"cl100k_base\")\n",
    "\n",
    "    # Calculate total number of tokens for all prompts\n",
    "    total_tokens = sum(df[column_name].apply(lambda x: num_tokens_from_string(f\"{prompt_text}\\n\\n{x}\", \"cl100k_base\")))\n",
    "\n",
    "    processed_tokens = 0\n",
    "\n",
    "    # Initialize tqdm with the initial description and total number of rows\n",
    "    pbar = tqdm(total=len(df), desc=\"Starting\")\n",
    "\n",
    "    for index, impression in df.iterrows():\n",
    "        # Constructing the full prompt with impression\n",
    "        full_prompt = f\"{prompt_text}\\n\\n{impression[column_name]}\" \n",
    "        \n",
    "        # Generating a single response for all conditions\n",
    "        response = create_gpt4_call(full_prompt)\n",
    "\n",
    "        # Count the number of tokens for the current full prompt\n",
    "        num_tokens = num_tokens_from_string(full_prompt, \"cl100k_base\")\n",
    "        processed_tokens += num_tokens\n",
    "        tokens_left = total_tokens - processed_tokens\n",
    "\n",
    "        # Update tqdm description to show both item progress and token count\n",
    "        pbar.set_description(f\"Classifying - {index + 1}/{len(df)} - Tokens Processed: {processed_tokens}, Tokens Left: {tokens_left}\")\n",
    "\n",
    "        # Update progress by one iteration for the item counter\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Parsing the response to extract labels for each condition\n",
    "        labels = response.split(\"\\n\")\n",
    "        for i, condition in enumerate(conditions):\n",
    "            label = labels[i].strip() if i < len(labels) else \"No\"\n",
    "            # Extract only the 'Yes' or 'No' part from the label\n",
    "            label = \"Yes\" if \"Yes\" in label else \"No\"\n",
    "            predictions[condition].append(label)\n",
    "   \n",
    "    # Save intermediate results every nth rows\n",
    "        if (index + 1) % num == 0:\n",
    "            for condition, condition_predictions in predictions.items():\n",
    "                processed_predictions = condition_predictions[:index + 1]\n",
    "\n",
    "                # Get indices of the rows to update\n",
    "                indices_to_update = df.index[:index + 1]\n",
    "\n",
    "                # Use .loc to update the original DataFrame\n",
    "                df.loc[indices_to_update, f'{condition}_predicted'] = np.where(np.array(processed_predictions) == 'Yes', 1, 0)\n",
    "\n",
    "            # Save the updated part of the DataFrame\n",
    "            df.iloc[:index + 1].to_csv(path_results + f'output_at_row_{index + 1}.csv', index=False)\n",
    "\n",
    "        \n",
    "    # Convert 'Yes'/'No' labels to binary (1/0) and add to DataFrame\n",
    "    for condition, condition_predictions in predictions.items():\n",
    "        df[f'{condition}_predicted'] = np.where(np.array(condition_predictions) == 'Yes', 1, 0)\n",
    "        \n",
    "\n",
    "    pbar.close()\n",
    "    return df, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f69f7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying - 1680/2402 - Tokens Processed: 313146, Tokens Left: 131566:  70%|██████▉   | 1680/2402 [1:18:35<30:56,  2.57s/it]  "
     ]
    }
   ],
   "source": [
    "df, predictions = classify_impressions(df_test,'final_deid', conditions, prompt_text, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert predictions into dataframe\n",
    "\n",
    "# predictions = pd.DataFrame(_)\n",
    "\n",
    "# # Function to clean the data\n",
    "# def clean_data(entry):\n",
    "#     if 'yes' in entry.lower():\n",
    "#         return 'Yes'\n",
    "#     elif 'no' in entry.lower():\n",
    "#         return 'No'\n",
    "#     else:\n",
    "#         return None\n",
    "    \n",
    "# predictions = predictions.applymap(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_cols = [col for col in classified_df.columns if col.endswith('_predicted')]\n",
    "true_label_cols = classified_df.columns[3:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label_cols, predicted_label_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de979a04",
   "metadata": {
    "id": "de979a04"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_performance(df, true_label_cols, predicted_label_cols):\n",
    "    for true_label_col, predicted_label_col in zip(true_label_cols, predicted_label_cols):\n",
    "        # Extract true and predicted labels\n",
    "        true_labels = df[true_label_col]\n",
    "        predicted_labels = df[predicted_label_col]\n",
    "\n",
    "        # Compute the evaluation metrics\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        precision = precision_score(true_labels, predicted_labels)\n",
    "        recall = recall_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Evaluation Metrics for {predicted_label_col}:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296b56d",
   "metadata": {
    "id": "3296b56d"
   },
   "outputs": [],
   "source": [
    "evaluate_model_performance(classified_df, true_label_cols, predicted_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for true_col, pred_col in zip(true_label_cols, predicted_label_cols):\n",
    "    classified_df['results_' + true_col] = np.where(classified_df[true_col] == classified_df[pred_col], \"Correct\", \"Incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf12ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582903e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df.to_csv(\"classified_df_200.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
