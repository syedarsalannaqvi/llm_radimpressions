{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9221915c",
   "metadata": {
    "id": "9221915c"
   },
   "outputs": [],
   "source": [
    "# Import all the required packages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import sidetable\n",
    "import time\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be15a5",
   "metadata": {
    "id": "a6be15a5"
   },
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with your API key\n",
    "api_key = '..'\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a271d8e",
   "metadata": {
    "id": "3a271d8e"
   },
   "outputs": [],
   "source": [
    "# Path to the data files\n",
    "path = r'...'\n",
    "\n",
    "# Creating a list of all the paths for files in different experiments\n",
    "paths = []\n",
    "for subfolders in os.listdir(path):\n",
    "    x = os.path.join(path, subfolders)\n",
    "    paths.append(x)\n",
    "    \n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9f723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the sampled datafile for 100 patients\n",
    "df_test = pd.read_csv(paths[0] + r'\\df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5ae0f9",
   "metadata": {
    "id": "2f5ae0f9"
   },
   "outputs": [],
   "source": [
    "df_test['final_deid'] = df_test['final_deid'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3fa528",
   "metadata": {
    "id": "4b3fa528"
   },
   "outputs": [],
   "source": [
    "def remove_substring(text, substring):\n",
    "    index = text.find(substring)\n",
    "    if index != -1:\n",
    "        return text[:index]  # Return the text up to the found index\n",
    "    else:\n",
    "        return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe23389",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = \"I, the teaching physician\"\n",
    "df_test['final_deid'] = df_test['final_deid'].apply(lambda x: remove_substring(x, remove))\n",
    "\n",
    "remove = \"ATTESTATION\"\n",
    "df_test['final_deid'] = df_test['final_deid'].apply(lambda x: remove_substring(x, remove))\n",
    "\n",
    "remove = \"Critical results were communicated\"\n",
    "df_test['final_deid'] = df_test['final_deid'].apply(lambda x: remove_substring(x, remove))\n",
    "\n",
    "remove = \"Electronically Signed by \"\n",
    "df_test['final_deid'] = df_test['final_deid'].apply(lambda x: remove_substring(x, remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ac28128",
   "metadata": {
    "id": "7ac28128"
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "#     encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "924cbb71",
   "metadata": {
    "id": "924cbb71"
   },
   "outputs": [],
   "source": [
    "# Encoding name for the  model\n",
    "# encoding_name = \"gpt-4\" ## if encoding for a specific model is used\n",
    "encoding_name = 'cl100k_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce1d5ee",
   "metadata": {
    "id": "fce1d5ee",
    "outputId": "7b9d702b-be49-47d3-dd70-1c9f39ee7041"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the dataset: 187698\n"
     ]
    }
   ],
   "source": [
    "# Now calculate the number of tokens for each truncated string\n",
    "df_test['num_tokens'] = df_test['final_deid'].apply(lambda x: num_tokens_from_string(x, encoding_name))\n",
    "\n",
    "# Calculate the total number of tokens\n",
    "total_tokens = df_test['num_tokens'].sum()\n",
    "print(f\"Total number of tokens in the dataset: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a208d47c",
   "metadata": {},
   "source": [
    "### Fewshot with baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05faf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"\"\" Identify if the following radiology impression text indicates outcomes: (1) any cancer, (2) progression/worsening, (3) response/improvement, (4) brain metastases, (5) bone/osseous metastases, (6) adrenal metastases, (7) liver/hepatic metastases, (8) lung/pulmonary metastases, (9) lymph node/nodal metastases, (10) peritoneal metastases. Answer in Yes or No. Do not give an explanation.\n",
    "\n",
    "EXAMPLE: \n",
    "An example of impression and output is given below:\n",
    "\n",
    "IMPRESSION: \n",
    "...\n",
    "\n",
    "1.\tany cancer: Yes \n",
    "2.\tprogression: Yes\n",
    "3.\tresponse/improvement: No\n",
    "4.\tbrain metastases: No \n",
    "5.\tbone/osseous metastases: No \n",
    "6.\tadrenal metastases: No \n",
    "7.\tliver/hepatic metastases: No \n",
    "8.\tFor lung/pulmonary metastases: Yes\n",
    "9.\tFor lymph node/nodal metastases: Yes\n",
    "10.\tFor peritoneal metastases: No \"\"\"\n",
    "\n",
    "num_tokens_from_string(prompt_text, encoding_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2cc9124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1102860"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['total_num_tokens'] = df_test['num_tokens'] + 381\n",
    "df_test['total_num_tokens'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268f7c5",
   "metadata": {
    "id": "0268f7c5"
   },
   "outputs": [],
   "source": [
    "# Function to create a single GPT-4 API call\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(10))\n",
    "def create_gpt4_call(content):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"model_name (gpt4/gpt4o)\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to analyze radiology reports.\"},\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        temperature=1e-12,\n",
    "        max_tokens=2048,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cc20c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of conditions to be checked\n",
    "conditions = [\"any cancer\", \"progression/worsening\", \"response/improvement\", \n",
    "                  \"brain metastases\", \"bone/osseous metastases\", \"adrenal metastases\", \n",
    "                  \"liver/hepatic metastases\", \"lung/pulmonary metastases\", \n",
    "                  \"lymph node/nodal metastases\", \"peritoneal metastases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fca447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_results = r'...'\n",
    "\n",
    "# Function to classify impressions for the fixed set of conditions\n",
    "def classify_impressions(df, column_name, conditions, prompt_text, num):\n",
    "    \n",
    "    # Reset the index of the DataFrame before running the loop. \n",
    "    # This will ensure that the DataFrame has a simple integer-based index which should align with loop's index variable.\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Initialize a dictionary to store predictions\n",
    "    predictions = {condition: [] for condition in conditions}\n",
    "    \n",
    "    # Calculate token count for the static part of the prompt\n",
    "    static_prompt_tokens = num_tokens_from_string(prompt_text, \"cl100k_base\")\n",
    "\n",
    "    # Calculate total number of tokens for all prompts\n",
    "    total_tokens = sum(df[column_name].apply(lambda x: num_tokens_from_string(f\"{prompt_text}\\n\\n{x}\", \"cl100k_base\")))\n",
    "\n",
    "    processed_tokens = 0\n",
    "\n",
    "    # Initialize tqdm with the initial description and total number of rows\n",
    "    pbar = tqdm(total=len(df), desc=\"Starting\")\n",
    "\n",
    "    for index, impression in df.iterrows():\n",
    "        # Constructing the full prompt with impression\n",
    "        full_prompt = f\"{prompt_text}\\n\\n{impression[column_name]}\" \n",
    "        \n",
    "        # Generating a single response for all conditions\n",
    "        response = create_gpt4_call(full_prompt)\n",
    "\n",
    "        # Count the number of tokens for the current full prompt\n",
    "        num_tokens = num_tokens_from_string(full_prompt, \"cl100k_base\")\n",
    "        processed_tokens += num_tokens\n",
    "        tokens_left = total_tokens - processed_tokens\n",
    "\n",
    "        # Update tqdm description to show both item progress and token count\n",
    "        pbar.set_description(f\"Classifying - {index + 1}/{len(df)} - Tokens Processed: {processed_tokens}, Tokens Left: {tokens_left}\")\n",
    "\n",
    "        # Update progress by one iteration for the item counter\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Parsing the response to extract labels for each condition\n",
    "        labels = response.split(\"\\n\")\n",
    "        for i, condition in enumerate(conditions):\n",
    "            label = labels[i].strip() if i < len(labels) else \"No\"\n",
    "            # Extract only the 'Yes' or 'No' part from the label\n",
    "            label = \"Yes\" if \"Yes\" in label else \"No\"\n",
    "            predictions[condition].append(label)\n",
    "   \n",
    "    # Save intermediate results every nth rows\n",
    "        if (index + 1) % num == 0:\n",
    "            for condition, condition_predictions in predictions.items():\n",
    "                processed_predictions = condition_predictions[:index + 1]\n",
    "\n",
    "                # Get indices of the rows to update\n",
    "                indices_to_update = df.index[:index + 1]\n",
    "\n",
    "                # Use .loc to update the original DataFrame\n",
    "                df.loc[indices_to_update, f'{condition}_predicted'] = np.where(np.array(processed_predictions) == 'Yes', 1, 0)\n",
    "\n",
    "            # Save the updated part of the DataFrame\n",
    "            df.iloc[:index + 1].to_csv(path_results + f'output_at_row_{index + 1}.csv', index=False)\n",
    "\n",
    "        \n",
    "    # Convert 'Yes'/'No' labels to binary (1/0) and add to DataFrame\n",
    "    for condition, condition_predictions in predictions.items():\n",
    "        df[f'{condition}_predicted'] = np.where(np.array(condition_predictions) == 'Yes', 1, 0)\n",
    "        \n",
    "\n",
    "    pbar.close()\n",
    "    return df, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f69f7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying - 2402/2402 - Tokens Processed: 1105262, Tokens Left: 0: 100%|██████████| 2402/2402 [2:43:24<00:00,  4.08s/it]      \n"
     ]
    }
   ],
   "source": [
    "df, predictions = classify_impressions(df_test,'final_deid', conditions, prompt_text, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcd19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31f9ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_cols = [col for col in classified_df.columns if col.endswith('_predicted')]\n",
    "true_label_cols = classified_df.columns[3:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c30a118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['any_cancer', 'progression', 'response', 'brain_met', 'bone_met',\n",
       "        'adrenal_met', 'liver_met', 'lung_met', 'node_met', 'peritoneal_met'],\n",
       "       dtype='object'),\n",
       " ['any cancer_predicted',\n",
       "  'progression/worsening_predicted',\n",
       "  'response/improvement_predicted',\n",
       "  'brain metastases_predicted',\n",
       "  'bone/osseous metastases_predicted',\n",
       "  'adrenal metastases_predicted',\n",
       "  'liver/hepatic metastases_predicted',\n",
       "  'lung/pulmonary metastases_predicted',\n",
       "  'lymph node/nodal metastases_predicted',\n",
       "  'peritoneal metastases_predicted'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label_cols, predicted_label_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de979a04",
   "metadata": {
    "id": "de979a04"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_performance(df, true_label_cols, predicted_label_cols):\n",
    "    for true_label_col, predicted_label_col in zip(true_label_cols, predicted_label_cols):\n",
    "        # Extract true and predicted labels\n",
    "        true_labels = df[true_label_col]\n",
    "        predicted_labels = df[predicted_label_col]\n",
    "\n",
    "        # Compute the evaluation metrics\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        precision = precision_score(true_labels, predicted_labels)\n",
    "        recall = recall_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Evaluation Metrics for {predicted_label_col}:\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Precision: {precision:.4f}\")\n",
    "        print(f\"  Recall: {recall:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3296b56d",
   "metadata": {
    "id": "3296b56d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for any cancer_predicted:\n",
      "  Accuracy: 0.8564\n",
      "  Precision: 0.8346\n",
      "  Recall: 0.9346\n",
      "  F1 Score: 0.8818\n",
      "\n",
      "Evaluation Metrics for progression/worsening_predicted:\n",
      "  Accuracy: 0.8418\n",
      "  Precision: 0.6067\n",
      "  Recall: 0.8606\n",
      "  F1 Score: 0.7117\n",
      "\n",
      "Evaluation Metrics for response/improvement_predicted:\n",
      "  Accuracy: 0.7573\n",
      "  Precision: 0.2058\n",
      "  Recall: 0.9740\n",
      "  F1 Score: 0.3398\n",
      "\n",
      "Evaluation Metrics for brain metastases_predicted:\n",
      "  Accuracy: 0.9713\n",
      "  Precision: 0.8063\n",
      "  Recall: 0.7725\n",
      "  F1 Score: 0.7890\n",
      "\n",
      "Evaluation Metrics for bone/osseous metastases_predicted:\n",
      "  Accuracy: 0.9534\n",
      "  Precision: 0.8619\n",
      "  Recall: 0.9267\n",
      "  F1 Score: 0.8931\n",
      "\n",
      "Evaluation Metrics for adrenal metastases_predicted:\n",
      "  Accuracy: 0.9858\n",
      "  Precision: 0.6916\n",
      "  Recall: 0.9867\n",
      "  F1 Score: 0.8132\n",
      "\n",
      "Evaluation Metrics for liver/hepatic metastases_predicted:\n",
      "  Accuracy: 0.9613\n",
      "  Precision: 0.7515\n",
      "  Recall: 0.9658\n",
      "  F1 Score: 0.8453\n",
      "\n",
      "Evaluation Metrics for lung/pulmonary metastases_predicted:\n",
      "  Accuracy: 0.8976\n",
      "  Precision: 0.6432\n",
      "  Recall: 0.8949\n",
      "  F1 Score: 0.7485\n",
      "\n",
      "Evaluation Metrics for lymph node/nodal metastases_predicted:\n",
      "  Accuracy: 0.9151\n",
      "  Precision: 0.5987\n",
      "  Recall: 0.9356\n",
      "  F1 Score: 0.7302\n",
      "\n",
      "Evaluation Metrics for peritoneal metastases_predicted:\n",
      "  Accuracy: 0.9842\n",
      "  Precision: 0.6304\n",
      "  Recall: 0.9355\n",
      "  F1 Score: 0.7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_performance(classified_df, true_label_cols, predicted_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af0e7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for true_col, pred_col in zip(true_label_cols, predicted_label_cols):\n",
    "    classified_df['results_' + true_col] = np.where(classified_df[true_col] == classified_df[pred_col], \"Correct\", \"Incorrect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "582903e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df.to_csv(path_results + \"classified_df_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
